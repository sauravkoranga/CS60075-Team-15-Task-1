{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "color-subject",
   "metadata": {
    "id": "color-subject"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import warnings\n",
    "import re\n",
    "import csv\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annual-building",
   "metadata": {
    "id": "annual-building"
   },
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "first-nurse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert  \n",
    "def listToString(s): \n",
    "    \n",
    "    # initialize an empty string\n",
    "    str1 = \"\" \n",
    "    \n",
    "    # traverse in the string  \n",
    "    for ele in s: \n",
    "        str1 += ele\n",
    "        str1 += \" \"\n",
    "    \n",
    "    # return string  \n",
    "    return str1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "WCV6z_baH0CG",
   "metadata": {
    "id": "WCV6z_baH0CG"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(sen):\n",
    "    \n",
    "    # lemmatization function\n",
    "    lemma = WordNetLemmatizer()\n",
    "\n",
    "    # collecting stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Removing html tags\n",
    "    sentence = remove_tags(sen)\n",
    "    \n",
    "    # Removing brackets\n",
    "    sentence = re.sub('\\[[^]]*]', '', sentence)\n",
    "\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "\n",
    "    # Single character removal\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "    \n",
    "    # Removing punctuations\n",
    "\n",
    "    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "    sentence = tokenizer.tokenize(sentence)\n",
    "\n",
    "    # converting to lower case\n",
    "\n",
    "    sentence = [word.lower() for word in sentence]\n",
    "\n",
    "    # .........removing stop words and perform lemmatization.............\n",
    "    tkn = []\n",
    "    for w in sentence:\n",
    "        if not w in stop_words:\n",
    "            x = lemma.lemmatize(w)\n",
    "            tkn.append(w)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    return listToString(tkn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "v4V5TZ1cH4DG",
   "metadata": {
    "id": "v4V5TZ1cH4DG"
   },
   "outputs": [],
   "source": [
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handmade-plumbing",
   "metadata": {},
   "source": [
    "# Single Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "prescribed-louisville",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "portable-essence",
    "outputId": "44584b47-58f0-478b-dd1a-46a8fda8ca03"
   },
   "outputs": [],
   "source": [
    "df_train1 = pd.read_csv(\"Dataset/Sub-task 1/lcp_single_train.tsv\",\n",
    "                              delimiter=\"\\t\", quoting=csv.QUOTE_NONE, encoding='utf-8')\n",
    "df_train2 = pd.read_csv(\"Dataset/Sub-task 1/lcp_single_trial.tsv\",\n",
    "                              delimiter=\"\\t\", quoting=csv.QUOTE_NONE, encoding='utf-8')\n",
    "df_train2 = df_train2.rename(columns = {\"subcorpus\":\"corpus\"})\n",
    "df_train = df_train2.append(df_train1, ignore_index=True)\n",
    "label = df_train.columns[len(df_train.columns)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sss36HgfH-VJ",
   "metadata": {
    "id": "sss36HgfH-VJ"
   },
   "outputs": [],
   "source": [
    "X_sen_train = []\n",
    "sentences = list(df_train['sentence'])\n",
    "for sen in sentences:\n",
    "    X_sen_train.append(preprocess_text(sen))\n",
    "X_tok_train = []\n",
    "y_sen_train = []\n",
    "\n",
    "df_train['token'] = df_train['token'].astype(str)\n",
    "sentences = list(df_train['token'])\n",
    "for sen in sentences:\n",
    "    X_tok_train.append(preprocess_text(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aquatic-cambridge",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "aquatic-cambridge",
    "outputId": "c908f243-274e-49eb-e03f-b9d247ae6728"
   },
   "outputs": [],
   "source": [
    "y_sen_train = df_train['complexity'].tolist()\n",
    "df_test = pd.read_csv(\"Dataset/Sub-task 1/lcp_single_test_labels.tsv\",\n",
    "                              delimiter=\"\\t\", quoting=csv.QUOTE_NONE, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "nWNtj23lKNtg",
   "metadata": {
    "id": "nWNtj23lKNtg"
   },
   "outputs": [],
   "source": [
    "X_sen_test = []\n",
    "sentences = list(df_test['sentence'])\n",
    "for sen in sentences:\n",
    "    X_sen_test.append(preprocess_text(sen))\n",
    "X_tok_test = []\n",
    "y_sen_test = []\n",
    "\n",
    "df_test['token'] = df_test['token'].astype(str)\n",
    "sentences = list(df_test['token'])\n",
    "\n",
    "for sen in sentences:\n",
    "    X_tok_test.append(preprocess_text(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "Byf1IgtSKoA8",
   "metadata": {
    "id": "Byf1IgtSKoA8"
   },
   "outputs": [],
   "source": [
    "y_sen_test = df_test['complexity'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sufficient-asthma",
   "metadata": {},
   "source": [
    "## Universal Sentence Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "AuZ5t6i-u4U7",
   "metadata": {
    "id": "AuZ5t6i-u4U7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow-gpu in /home/sauravkoranga/.local/lib/python3.8/site-packages (2.4.1)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /home/sauravkoranga/.local/lib/python3.8/site-packages (from tensorflow-gpu) (3.15.3)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /home/sauravkoranga/.local/lib/python3.8/site-packages (from tensorflow-gpu) (0.2.0)\n",
      "Requirement already satisfied: gast==0.3.3 in /home/sauravkoranga/.local/lib/python3.8/site-packages (from tensorflow-gpu) (0.3.3)\n",
      "Requirement already satisfied: h5py~=2.10.0 in /home/sauravkoranga/.local/lib/python3.8/site-packages (from tensorflow-gpu) (2.10.0)\n",
      "Requirement already satisfied: tensorboard~=2.4 in /home/sauravkoranga/.local/lib/python3.8/site-packages (from tensorflow-gpu) (2.4.1)\n",
      "Requirement already satisfied: absl-py~=0.10 in /home/sauravkoranga/.local/lib/python3.8/site-packages (from tensorflow-gpu) (0.11.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /home/sauravkoranga/.local/lib/python3.8/site-packages (from tensorflow-gpu) (1.6.3)\n",
      "Requirement already satisfied: wheel~=0.35 in /home/sauravkoranga/.local/lib/python3.8/site-packages (from tensorflow-gpu) (0.35.1)\n",
      "Requirement already satisfied: grpcio~=1.32.0 in /home/sauravkoranga/.local/lib/python3.8/site-packages (from tensorflow-gpu) (1.32.0)\n",
      "Requirement already satisfied: six~=1.15.0 in /home/sauravkoranga/.local/lib/python3.8/site-packages (from tensorflow-gpu) (1.15.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /home/sauravkoranga/.local/lib/python3.8/site-packages (from tensorflow-gpu) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.8/dist-packages (from tensorflow-gpu) (3.7.4.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /home/sauravkoranga/.local/lib/python3.8/site-packages (from tensorflow-gpu) (2.4.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /home/sauravkoranga/.local/lib/python3.8/site-packages (from tensorflow-gpu) (1.1.2)\n",
      "Requirement already satisfied: numpy~=1.19.2 in /home/sauravkoranga/.local/lib/python3.8/site-packages (from tensorflow-gpu) (1.19.5)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /home/sauravkoranga/.local/lib/python3.8/site-packages (from tensorflow-gpu) (1.12)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /home/sauravkoranga/.local/lib/python3.8/site-packages (from tensorflow-gpu) (1.1.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /home/sauravkoranga/.local/lib/python3.8/site-packages (from tensorflow-gpu) (1.12.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/sauravkoranga/.local/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow-gpu) (1.27.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/sauravkoranga/.local/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow-gpu) (0.4.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/sauravkoranga/.local/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow-gpu) (1.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/lib/python3/dist-packages (from tensorboard~=2.4->tensorflow-gpu) (2.22.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/sauravkoranga/.local/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow-gpu) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/sauravkoranga/.local/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow-gpu) (1.8.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/sauravkoranga/.local/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow-gpu) (50.3.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu) (4.1.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/sauravkoranga/.local/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/sauravkoranga/.local/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/sauravkoranga/.local/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow-gpu) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/sauravkoranga/.local/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow-gpu) (3.1.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow-hub in /home/sauravkoranga/.local/lib/python3.8/site-packages (0.11.0)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /home/sauravkoranga/.local/lib/python3.8/site-packages (from tensorflow-hub) (3.15.3)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/sauravkoranga/.local/lib/python3.8/site-packages (from tensorflow-hub) (1.19.5)\n",
      "Requirement already satisfied: six>=1.9 in /home/sauravkoranga/.local/lib/python3.8/site-packages (from protobuf>=3.8.0->tensorflow-hub) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --upgrade tensorflow-gpu\n",
    "# Install TF-Hub.\n",
    "!pip3 install tensorflow-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "n7mXRXN4vKyJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n7mXRXN4vKyJ",
    "outputId": "c8cb42d2-1a92-443b-92a3-5ed6649e4964"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module https://tfhub.dev/google/universal-sentence-encoder/4 loaded\n"
     ]
    }
   ],
   "source": [
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\" \n",
    "model = hub.load(module_url)\n",
    "print (\"module %s loaded\" % module_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinated-evans",
   "metadata": {},
   "source": [
    "## Extracting Features for training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "_wSmdAdYvV_s",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "_wSmdAdYvV_s",
    "outputId": "d3b01085-e647-40b0-cd74-b00ca898474c"
   },
   "outputs": [],
   "source": [
    "x_sen_train = []\n",
    "for line in X_sen_train:\n",
    "  x = model([line])[0]\n",
    "  x.numpy()\n",
    "  x_sen_train.append(x)\n",
    "\n",
    "x_tok_train = []\n",
    "for line in X_tok_train:\n",
    "  x = model([line])[0]\n",
    "  x.numpy()\n",
    "  x_tok_train.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "YpP_fWtoyy2r",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YpP_fWtoyy2r",
    "outputId": "c9778862-b787-4cf0-8c62-189c7e2ded5c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8083, 512)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_sen_train = np.array(x_sen_train)\n",
    "x_tok_train = np.array(x_tok_train)\n",
    "x_sen_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "responsible-architect",
   "metadata": {},
   "source": [
    "## Extracting Features for testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "-E_qs2YrL13f",
   "metadata": {
    "id": "-E_qs2YrL13f"
   },
   "outputs": [],
   "source": [
    "x_sen_test = []\n",
    "for line in X_sen_test:\n",
    "  x = model([line])[0]\n",
    "  x.numpy()\n",
    "  x_sen_test.append(x)\n",
    "\n",
    "x_tok_test = []\n",
    "for line in X_tok_test:\n",
    "  x = model([line])[0]\n",
    "  x.numpy()\n",
    "  x_tok_test.append(x)\n",
    "'''\n",
    "x_cor_test = []\n",
    "for line in X_cor_test:\n",
    "  x = model([line])[0]\n",
    "  x.numpy()\n",
    "  x_cor_test.append(x)\n",
    "'''\n",
    "\n",
    "x_sen_test = np.array(x_sen_test)\n",
    "x_tok_test = np.array(x_tok_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "polar-surrey",
   "metadata": {
    "id": "polar-surrey"
   },
   "outputs": [],
   "source": [
    "x_train = np.concatenate((x_sen_train, x_tok_train), axis=1)\n",
    "x_test = np.concatenate((x_sen_test, x_tok_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "hired-holocaust",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hired-holocaust",
    "outputId": "4c8ab54d-12a2-4464-f431-29a1e2354c26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***********************************************************************************\n",
      "\n",
      "Pearson corelation and p-value for Single word task: (0.6690901121350543, 4.433567314874437e-120)\n",
      "\n",
      "***********************************************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn import svm\n",
    "reg_model = svm.SVR()\n",
    "reg_model.fit(x_train, y_sen_train)\n",
    "y_pred = reg_model.predict(x_test)\n",
    "print('\\n***********************************************************************************\\n')\n",
    "print('Pearson corelation and p-value for Single word task:', pearsonr(y_sen_test, y_pred))\n",
    "print('\\n***********************************************************************************\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "trying-ozone",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving predictions\n",
    "warnings.filterwarnings('ignore')\n",
    "df_test['Predicted Complexity'] = 0.00000\n",
    "for i in range(len(df_test)):\n",
    "    df_test['Predicted Complexity'][i] = y_pred[i]\n",
    "\n",
    "df_final = df_test.drop(columns=['complexity','corpus','sentence','token'])\n",
    "df_final.to_csv('single_test_predictions.csv',index=False,header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varying-hungary",
   "metadata": {},
   "source": [
    "# Multi-Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "electrical-consciousness",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "portable-essence",
    "outputId": "44584b47-58f0-478b-dd1a-46a8fda8ca03"
   },
   "outputs": [],
   "source": [
    "df_train1 = pd.read_csv(\"Dataset/Sub-task 2/lcp_multi_train.tsv\",\n",
    "                              delimiter=\"\\t\", quoting=csv.QUOTE_NONE, encoding='utf-8')\n",
    "df_train2 = pd.read_csv(\"Dataset/Sub-task 2/lcp_multi_trial.tsv\",\n",
    "                              delimiter=\"\\t\", quoting=csv.QUOTE_NONE, encoding='utf-8')\n",
    "df_train2 = df_train2.rename(columns = {\"subcorpus\":\"corpus\"})\n",
    "df_train_multi = df_train2.append(df_train1, ignore_index=True)\n",
    "label = df_train_multi.columns[len(df_train_multi.columns)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ultimate-priest",
   "metadata": {
    "id": "sss36HgfH-VJ"
   },
   "outputs": [],
   "source": [
    "X_sen_train_multi = []\n",
    "sentences = list(df_train_multi['sentence'])\n",
    "for sen in sentences:\n",
    "    X_sen_train_multi.append(preprocess_text(sen))\n",
    "X_tok_train_multi = []\n",
    "y_sen_train_multi = []\n",
    "\n",
    "df_train_multi['token'] = df_train_multi['token'].astype(str)\n",
    "sentences = list(df_train_multi['token'])\n",
    "for sen in sentences:\n",
    "    X_tok_train_multi.append(preprocess_text(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "amino-degree",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "aquatic-cambridge",
    "outputId": "c908f243-274e-49eb-e03f-b9d247ae6728"
   },
   "outputs": [],
   "source": [
    "y_sen_train_multi = df_train_multi['complexity'].tolist()\n",
    "df_test_multi = pd.read_csv(\"Dataset/Sub-task 2/lcp_multi_test_labels.tsv\",\n",
    "                              delimiter=\"\\t\", quoting=csv.QUOTE_NONE, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "extended-working",
   "metadata": {
    "id": "nWNtj23lKNtg"
   },
   "outputs": [],
   "source": [
    "X_sen_test_multi = []\n",
    "sentences = list(df_test_multi['sentence'])\n",
    "for sen in sentences:\n",
    "    X_sen_test_multi.append(preprocess_text(sen))\n",
    "X_tok_test_multi = []\n",
    "y_sen_test_multi = []\n",
    "\n",
    "df_test_multi['token'] = df_test_multi['token'].astype(str)\n",
    "sentences = list(df_test_multi['token'])\n",
    "\n",
    "for sen in sentences:\n",
    "    X_tok_test_multi.append(preprocess_text(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "consistent-ambassador",
   "metadata": {
    "id": "Byf1IgtSKoA8"
   },
   "outputs": [],
   "source": [
    "y_sen_test_multi = df_test_multi['complexity'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upset-threat",
   "metadata": {},
   "source": [
    "## Extracting Features for training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "excellent-wrist",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "_wSmdAdYvV_s",
    "outputId": "d3b01085-e647-40b0-cd74-b00ca898474c"
   },
   "outputs": [],
   "source": [
    "x_sen_train_multi = []\n",
    "for line in X_sen_train_multi:\n",
    "  x = model([line])[0]\n",
    "  x.numpy()\n",
    "  x_sen_train_multi.append(x)\n",
    "\n",
    "x_tok_train_multi = []\n",
    "for line in X_tok_train_multi:\n",
    "  x = model([line])[0]\n",
    "  x.numpy()\n",
    "  x_tok_train_multi.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ordered-settle",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YpP_fWtoyy2r",
    "outputId": "c9778862-b787-4cf0-8c62-189c7e2ded5c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1616, 512)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_sen_train_multi = np.array(x_sen_train_multi)\n",
    "x_tok_train_multi = np.array(x_tok_train_multi)\n",
    "x_sen_train_multi.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deluxe-corpus",
   "metadata": {},
   "source": [
    "## Extracting Features for testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "buried-coupon",
   "metadata": {
    "id": "-E_qs2YrL13f"
   },
   "outputs": [],
   "source": [
    "x_sen_test_multi = []\n",
    "for line in X_sen_test_multi:\n",
    "  x = model([line])[0]\n",
    "  x.numpy()\n",
    "  x_sen_test_multi.append(x)\n",
    "\n",
    "x_tok_test_multi = []\n",
    "for line in X_tok_test_multi:\n",
    "  x = model([line])[0]\n",
    "  x.numpy()\n",
    "  x_tok_test_multi.append(x)\n",
    "\n",
    "x_sen_test_multi = np.array(x_sen_test_multi)\n",
    "x_tok_test_multi = np.array(x_tok_test_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "thrown-subscription",
   "metadata": {
    "id": "polar-surrey"
   },
   "outputs": [],
   "source": [
    "x_train_multi = np.concatenate((x_sen_train_multi, x_tok_train_multi), axis=1)\n",
    "x_test_multi = np.concatenate((x_sen_test_multi, x_tok_test_multi), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "similar-montgomery",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hired-holocaust",
    "outputId": "4c8ab54d-12a2-4464-f431-29a1e2354c26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***********************************************************************************\n",
      "\n",
      "Pearson corelation and p-value for Multi word task: (0.7781217089789983, 1.3233615138572167e-38)\n",
      "\n",
      "***********************************************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn import svm\n",
    "reg_model = svm.SVR()\n",
    "reg_model.fit(x_train_multi, y_sen_train_multi)\n",
    "y_pred_multi = reg_model.predict(x_test_multi)\n",
    "pearsonr(y_sen_test_multi, y_pred_multi)\n",
    "print('\\n***********************************************************************************\\n')\n",
    "print('Pearson corelation and p-value for Multi word task:', pearsonr(y_sen_test_multi, y_pred_multi))\n",
    "print('\\n***********************************************************************************\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "reliable-discussion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving predictions\n",
    "warnings.filterwarnings('ignore')\n",
    "df_test_multi['Predicted Complexity'] = 0.00000\n",
    "for i in range(len(df_test_multi)):\n",
    "    df_test_multi['Predicted Complexity'][i] = y_pred_multi[i]\n",
    "\n",
    "df_final_multi = df_test_multi.drop(columns=['complexity','corpus','sentence','token'])\n",
    "df_final_multi.to_csv('multi_test_predictions.csv',index=False,header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attached-noise",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "racial-memorabilia",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "code_model_512_embe.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
